{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a45bc2fc-3be7-490a-aee3-6072c970a6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "import torchtext\n",
    "\n",
    "from captum.attr import TokenReferenceBase\n",
    "from captum.attr import LayerIntegratedGradients\n",
    "from captum.attr import visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7e35d68-df80-4d52-8e83-1f6292ebdf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    use_cuda = False\n",
    "    if torch.cuda.is_available():\n",
    "        pass\n",
    "        # use_cuda = True\n",
    "    print('CUDA enabled:', use_cuda)\n",
    "\n",
    "    modelName = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(modelName)\n",
    "    print(model.config)\n",
    "    id2label = model.config.to_dict()['id2label']\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "    if tokenizer._pad_token is not None:\n",
    "        pad_token = tokenizer.pad_token\n",
    "        pad_token_id = tokenizer.encode(pad_token)\n",
    "        print(pad_token, pad_token_id)\n",
    "    else:\n",
    "        logging.error(\"Using pad_token, but it is not set yet.\")\n",
    "    # p_special_tokens(tokenizer)\n",
    "    print(tokenizer.decode(0))\n",
    "    print(tokenizer.decode(101))\n",
    "    print(tokenizer.decode(102))\n",
    "    print(tokenizer.decode(103))\n",
    "\n",
    "    token_reference = TokenReferenceBase(reference_token_idx=pad_token_id[0])\n",
    "    # vocab = torchtext.vocab.vocab(tokenizer.get_vocab())\n",
    "\n",
    "    def custom_forward(input_ids, attention_mask):\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "        # print(outputs)\n",
    "        # probs = torch.softmax(outputs.logits, dim=1)\n",
    "        # print(probs)\n",
    "        # label_idx = torch.argmax(probs, dim=1)\n",
    "        # print(label_idx)\n",
    "        # return label_idx.unsqueeze(0)\n",
    "\n",
    "    lig = LayerIntegratedGradients(custom_forward, model.get_input_embeddings())\n",
    "\n",
    "    samples = [('It was a fantastic performance !', 1), ('Best film ever', 1), ('Such a great show!', 1), ('It was a horrible movie', 0), ('I\\'ve never watched something as bad', 0), ('That is a terrible movie.', 0)]\n",
    "    vis_result = []\n",
    "\n",
    "\n",
    "    for sentence, label in samples:\n",
    "        print(sentence, label)\n",
    "        inputs = tokenizer(sentence,\n",
    "                           padding=True,\n",
    "                           truncation=True,\n",
    "                           max_length=512,\n",
    "                           return_tensors=\"pt\")\n",
    "        print(inputs)\n",
    "        model.zero_grad()\n",
    "        if use_cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        outputs = model(**inputs)\n",
    "        print(outputs)\n",
    "        probs = torch.softmax(outputs.logits, dim=1)\n",
    "        print(probs)\n",
    "        label_idx = torch.argmax(probs, dim=1)\n",
    "        print(label_idx)\n",
    "        print(\"inputs['input_ids'].shape:\", inputs['input_ids'].shape)\n",
    "        reference_indices = token_reference.generate_reference(inputs['input_ids'].shape[1], device=inputs['input_ids'].device).unsqueeze(0)\n",
    "\n",
    "        # for key in inputs:\n",
    "        #     inputs[key] = inputs[key].unsqueeze(0)\n",
    "        print(\"inputs['input_ids'].shape:\", inputs['input_ids'].shape)\n",
    "        print('reference_indices.shape:', reference_indices.shape)\n",
    "        print(inputs['input_ids'].dtype)\n",
    "        print(reference_indices.dtype)\n",
    "        # attributions_ig, delta = lig.attribute(inputs=(inputs['input_ids'], inputs['attention_mask']), baselines=reference_indices, target=label, n_steps=500, return_convergence_delta=True)\n",
    "        attributions_ig, delta = lig.attribute(inputs=inputs['input_ids'],\n",
    "                                               baselines=reference_indices,\n",
    "                                               additional_forward_args=inputs['attention_mask'],\n",
    "                                               target=label,\n",
    "                                               n_steps=10,\n",
    "                                               return_convergence_delta=True)\n",
    "\n",
    "        attributions = attributions_ig.sum(dim=2).squeeze(0)\n",
    "        attributions = attributions / torch.norm(attributions)\n",
    "        attributions = attributions.cpu().detach().numpy()\n",
    "\n",
    "        prob = torch.max(probs).item()\n",
    "        print(attributions)\n",
    "        print(probs)\n",
    "        print(id2label[label_idx.item()])\n",
    "        print(id2label[label])\n",
    "        print(pad_token)\n",
    "        print(attributions.sum())\n",
    "        print(len(sentence))\n",
    "        print(delta)\n",
    "\n",
    "        text = [tokenizer.decode(x) for x in inputs['input_ids']][0].split(' ')\n",
    "        # storing couple samples in an array for visualization purposes\n",
    "        vis_result.append(visualization.VisualizationDataRecord(attributions,\n",
    "                                                                prob,\n",
    "                                                                id2label[label_idx.item()],\n",
    "                                                                id2label[label],\n",
    "                                                                pad_token,\n",
    "                                                                attributions.sum(),\n",
    "                                                                text,\n",
    "                                                                delta))\n",
    "    _ = visualization.visualize_text(vis_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f313706c-f831-4ece-994e-95caadd4347a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA enabled: False\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f908f407c4b4afb968757c8e2bcecec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777b6917dec0499fb114fbd161cfd42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"finetuning_task\": \"sst-2\",\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"NEGATIVE\",\n",
      "    \"1\": \"POSITIVE\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"NEGATIVE\": 0,\n",
      "    \"POSITIVE\": 1\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3859ebc4caaa4d85b3302cad8a70e39d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386977002cc7495a85f84286ddf21651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD] [101, 0, 102]\n",
      "[PAD]\n",
      "[CLS]\n",
      "[SEP]\n",
      "[MASK]\n",
      "It was a fantastic performance ! 1\n",
      "{'input_ids': tensor([[  101,  2009,  2001,  1037, 10392,  2836,   999,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.3378,  4.6968]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor([[1.1919e-04, 9.9988e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1])\n",
      "inputs['input_ids'].shape: torch.Size([1, 8])\n",
      "inputs['input_ids'].shape: torch.Size([1, 8])\n",
      "reference_indices.shape: torch.Size([1, 8])\n",
      "torch.int64\n",
      "torch.int64\n",
      "[ 0.         -0.00144865 -0.0661637   0.06876944  0.71151029  0.03989503\n",
      "  0.13625756  0.68153239]\n",
      "tensor([[1.1919e-04, 9.9988e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "POSITIVE\n",
      "POSITIVE\n",
      "[PAD]\n",
      "1.5703523661532837\n",
      "32\n",
      "tensor([2.2318], dtype=torch.float64)\n",
      "Best film ever 1\n",
      "{'input_ids': tensor([[ 101, 2190, 2143, 2412,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.1864,  4.5468]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor([[1.6112e-04, 9.9984e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1])\n",
      "inputs['input_ids'].shape: torch.Size([1, 5])\n",
      "inputs['input_ids'].shape: torch.Size([1, 5])\n",
      "reference_indices.shape: torch.Size([1, 5])\n",
      "torch.int64\n",
      "torch.int64\n",
      "[0.         0.50519465 0.27452255 0.2323749  0.78448559]\n",
      "tensor([[1.6112e-04, 9.9984e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "POSITIVE\n",
      "POSITIVE\n",
      "[PAD]\n",
      "1.7965776857394182\n",
      "14\n",
      "tensor([0.7735], dtype=torch.float64)\n",
      "Such a great show! 1\n",
      "{'input_ids': tensor([[ 101, 2107, 1037, 2307, 2265,  999,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-4.3255,  4.6575]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor([[1.2551e-04, 9.9987e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1])\n",
      "inputs['input_ids'].shape: torch.Size([1, 7])\n",
      "inputs['input_ids'].shape: torch.Size([1, 7])\n",
      "reference_indices.shape: torch.Size([1, 7])\n",
      "torch.int64\n",
      "torch.int64\n",
      "[ 0.         -0.17366231  0.08661519  0.79097084 -0.05220031  0.25625082\n",
      "  0.5179913 ]\n",
      "tensor([[1.2551e-04, 9.9987e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "POSITIVE\n",
      "POSITIVE\n",
      "[PAD]\n",
      "1.4259655268414395\n",
      "18\n",
      "tensor([1.3169], dtype=torch.float64)\n",
      "It was a horrible movie 0\n",
      "{'input_ids': tensor([[ 101, 2009, 2001, 1037, 9202, 3185,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 4.5159, -3.7262]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor([[9.9974e-01, 2.6326e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0])\n",
      "inputs['input_ids'].shape: torch.Size([1, 7])\n",
      "inputs['input_ids'].shape: torch.Size([1, 7])\n",
      "reference_indices.shape: torch.Size([1, 7])\n",
      "torch.int64\n",
      "torch.int64\n",
      "[ 0.         -0.10531676 -0.09533858  0.22580703  0.94713671  0.05248865\n",
      " -0.17031477]\n",
      "tensor([[9.9974e-01, 2.6326e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "NEGATIVE\n",
      "NEGATIVE\n",
      "[PAD]\n",
      "0.8544622725060442\n",
      "23\n",
      "tensor([9.2712], dtype=torch.float64)\n",
      "I've never watched something as bad 0\n",
      "{'input_ids': tensor([[ 101, 1045, 1005, 2310, 2196, 3427, 2242, 2004, 2919,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[-1.6823,  1.7135]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor([[0.0324, 0.9676]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1])\n",
      "inputs['input_ids'].shape: torch.Size([1, 10])\n",
      "inputs['input_ids'].shape: torch.Size([1, 10])\n",
      "reference_indices.shape: torch.Size([1, 10])\n",
      "torch.int64\n",
      "torch.int64\n",
      "[ 0.         -0.08636675  0.08185583  0.13341283  0.82047874 -0.26330148\n",
      "  0.05245204  0.19210407 -0.3152271  -0.29411746]\n",
      "tensor([[0.0324, 0.9676]], grad_fn=<SoftmaxBackward0>)\n",
      "POSITIVE\n",
      "NEGATIVE\n",
      "[PAD]\n",
      "0.3212907156175085\n",
      "35\n",
      "tensor([2.5307], dtype=torch.float64)\n",
      "That is a terrible movie. 0\n",
      "{'input_ids': tensor([[ 101, 2008, 2003, 1037, 6659, 3185, 1012,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 4.4044, -3.6310]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "tensor([[9.9968e-01, 3.2370e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0])\n",
      "inputs['input_ids'].shape: torch.Size([1, 8])\n",
      "inputs['input_ids'].shape: torch.Size([1, 8])\n",
      "reference_indices.shape: torch.Size([1, 8])\n",
      "torch.int64\n",
      "torch.int64\n",
      "[ 0.         -0.05948265 -0.05337889  0.17176102  0.93296073  0.0662362\n",
      " -0.03477499 -0.29681376]\n",
      "tensor([[9.9968e-01, 3.2370e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "NEGATIVE\n",
      "NEGATIVE\n",
      "[PAD]\n",
      "0.7265076577793033\n",
      "25\n",
      "tensor([2.7250], dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>POSITIVE</b></text></td><td><text style=\"padding-right:2em\"><b>POSITIVE (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>[PAD]</b></text></td><td><text style=\"padding-right:2em\"><b>1.57</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 65%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> fantastic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> performance!                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>POSITIVE</b></text></td><td><text style=\"padding-right:2em\"><b>POSITIVE (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>[PAD]</b></text></td><td><text style=\"padding-right:2em\"><b>1.80</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> film                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ever                    </font></mark><mark style=\"background-color: hsl(120, 75%, 61%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>POSITIVE</b></text></td><td><text style=\"padding-right:2em\"><b>POSITIVE (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>[PAD]</b></text></td><td><text style=\"padding-right:2em\"><b>1.43</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> such                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 61%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> great                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> show!                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>NEGATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>NEGATIVE (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>[PAD]</b></text></td><td><text style=\"padding-right:2em\"><b>0.85</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> was                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 53%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> horrible                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie                    </font></mark><mark style=\"background-color: hsl(0, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>NEGATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>POSITIVE (0.97)</b></text></td><td><text style=\"padding-right:2em\"><b>[PAD]</b></text></td><td><text style=\"padding-right:2em\"><b>0.32</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> i've                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> never                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> watched                    </font></mark><mark style=\"background-color: hsl(120, 75%, 59%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> something                    </font></mark><mark style=\"background-color: hsl(0, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> bad                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr><tr><td><text style=\"padding-right:2em\"><b>NEGATIVE</b></text></td><td><text style=\"padding-right:2em\"><b>NEGATIVE (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>[PAD]</b></text></td><td><text style=\"padding-right:2em\"><b>0.73</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 54%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> terrible                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> movie.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992bf6bb-4f5b-495f-a1ec-30507185549b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
